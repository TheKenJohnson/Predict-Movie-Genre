{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhq4HZ-G04dj"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Predicting Movie Genre Based on Plot Synopsis using NLP and Softmax**\n",
        "**Ken Johnson**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nk3V3fs1N0B"
      },
      "source": [
        "### **1. Overview and Objective**\n",
        "In this assignment, you will be training logistic regression via stochastic gradient-based optimization for predicting income from census data having various features such as age, employment information, education, marital status, occupation, race, sex, capital gain or loss, hours per week, country, etc.\n",
        "\n",
        "Using the trained model, you will predict the probability that a person earns more than $50k per year. As such, this assignment involves end-to-end training and inference using logistic regression. You will get hands-on understanding of maximizing the conditional log likelihood while incorporating regularization (i.e., MAP estimation) for learning the parameters of logistic regression (i.e, model's weights). You will also get a chance to perform feature engineering and additional fine-tuning of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqbuYSDzqv1r"
      },
      "source": [
        "### **2. Logistic Function [5 points]**###\n",
        "\n",
        "To perform logistic regression, you have to be able to calculate the logistic function defined as follows:\n",
        "\n",
        "$$a = \\frac{1}{1+e^{-b}}$$\n",
        "\n",
        "Fill in the `logistic` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_5tuRhuyr7WE"
      },
      "outputs": [],
      "source": [
        "from math import exp\n",
        "import random\n",
        "random.seed(1)\n",
        "\n",
        "# TODO: Calculate logistic\n",
        "def logistic(x):\n",
        "    s = (1/ (1 + exp(-x)))\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UBGYS_bWzQZg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass: Logistic Function [5 points]\n"
          ]
        }
      ],
      "source": [
        "def test_logistic():\n",
        "    assert abs(logistic(1) - 0.7310585786300049) < 1e-7\n",
        "    assert abs(logistic(2) - 0.8807970779778823) < 1e-7\n",
        "    assert abs(logistic(-1) - 0.2689414213699951) < 1e-7\n",
        "test_logistic()\n",
        "print('Pass: Logistic Function [5 points]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJpYnuANsDZy"
      },
      "source": [
        "### **3. Dot Product [10 points]**###\n",
        "\n",
        "The model you are training is just a bunch of numerical weights. To run your model on a data points you will need to compute the dot product of your weights and the features for that data point and run the result through your logistic function. The dot product of two vectors $\\mathbf a = [a_1, a_2, ..., a_n]$ and $\\mathbf b = [b_1, b_2, ..., b_n]$ is defined as:\n",
        "\n",
        "$\\mathbf a\\mathbf \\cdot \\mathbf b = \\sum_{i=1}^n a_i b_i = a_1b_1 + a_2 b_2 + ... + a_n b_n$\n",
        "\n",
        "Fill in the `dot` function to compute the dot product of two vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "St-LgcKEsFyY"
      },
      "outputs": [],
      "source": [
        "# TODO: Calculate dot product of two lists\n",
        "def dot(x, y):\n",
        "    s = 0\n",
        "    for i in range(len(x)):\n",
        "        s+= x[i] * y[i]\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VJudSpfIztRH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass: Dot Product [10 points]\n"
          ]
        }
      ],
      "source": [
        "def test_dot():\n",
        "    d = dot([1.1,2,3.5], [-1,0.1,.08])\n",
        "    assert abs(d - (-.62)) < 1e-7\n",
        "test_dot()\n",
        "print('Pass: Dot Product [10 points]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rr1jtjmsXy9"
      },
      "source": [
        "### **4. Prediction [5 points]**###\n",
        "\n",
        "Now that you can calculate the dot product, prediction task for new data points is straightforward given a model (i.e., the model's weights are available).\n",
        "\n",
        "To predict for data new points, compute the dot product of your model's weights and the corresponding feature and run the result through your logistic function.\n",
        "\n",
        "Fill in the `predict` function for prediction task for a new data point given a model. Take a look at `test_predict()` to see what the format for data points is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vf5HqXzWstBA"
      },
      "outputs": [],
      "source": [
        "# TODO: Calculate prediction based on model\n",
        "def predict(model, point):\n",
        "    p = logistic(dot(model, point['features']))\n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "skm7Od300CcB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass: Prediction [5 points]\n"
          ]
        }
      ],
      "source": [
        "def test_predict():\n",
        "    model = [1,2,1,0,1]\n",
        "    point = {'features':[.4,1,3,.01,.1], 'label': 1}\n",
        "    p = predict(model, point)\n",
        "    assert abs(p - 0.995929862284) < 1e-7\n",
        "test_predict()\n",
        "print('Pass: Prediction [5 points]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VWBaiVhiNzw"
      },
      "source": [
        "### **5. Data Loading and Analysis [5 points]**###\n",
        "\n",
        "Cells below load the dataset from a dataset file. `data` is an array consists of several data points.\n",
        "We provide a list of print statements to help you understand the data format.\n",
        "Each point is an `ordered Dict` type and has `15` features. Features include *age*, *type_employer* and so on (see the following cell to get an idea on how the data look like).\n",
        "\n",
        "Performing basic data analysis may help you better understand the dataset and the features involved. As a rudimentary analysis, use the `age` attribute and report how many people fall into the age ranges of `(0, 20], (20, 40], (40, 60], (60, 80], (80, 100]`.\n",
        "Fill in the `calculate_age_bins` functions and return a list of five values representing the amounts in each age range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "noyeUdF99ckG"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NH_u_CJj9dHT"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_csv(filename):\n",
        "    lines = []\n",
        "    with open(filename) as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for line in reader:\n",
        "            lines.append(line)\n",
        "    return lines\n",
        "\n",
        "def load_adult_data(fn):\n",
        "    return load_csv(fn)\n",
        "\n",
        "# Note: Possibly use different data for training and validation to get a more accurate result,\n",
        "# but remember that in the last part your model will be trained on the full training data\n",
        "# load_adult_data() and be tested on a test dataset you don't have access to.\n",
        "def load_adult_train_data(fn):\n",
        "    return load_adult_data(fn)\n",
        "    \n",
        "\n",
        "def load_adult_valid_data(fn):\n",
        "    return load_adult_data(fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qV336G0TKg0Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to adult.data: adult_balanced.data\n"
          ]
        }
      ],
      "source": [
        "#Append the directory to your python path using sys\n",
        "import sys\n",
        "import os\n",
        "prefix = '/content/drive/My Drive/'\n",
        "# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n",
        "customized_path_to_your_homework = 'Colab Notebooks/'\n",
        "sys_path = prefix + customized_path_to_your_homework\n",
        "sys.path.append(sys_path)\n",
        "# print(sys.path)\n",
        "#loaded locally\n",
        "fp_data = os.path.join('adult_balanced.data')\n",
        "data = load_adult_train_data(fp_data)\n",
        "print('Path to adult.data: {}'.format(fp_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wwggTdsVsssK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "{'age': '26', 'type_employer': 'Private', 'fnlwgt': '162302', 'education': 'Some-college', 'education_num': '10', 'marital': 'Never-married', 'occupation': 'Machine-op-inspct', 'relationship': 'Not-in-family', 'race': 'Asian-Pac-Islander', 'sex': 'Male', 'capital_gain': '0', 'capital_loss': '0', 'hr_per_week': '40', 'country': 'Philippines', 'income': '<=50K'}\n",
            "15\n",
            "dict_keys(['age', 'type_employer', 'fnlwgt', 'education', 'education_num', 'marital', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hr_per_week', 'country', 'income'])\n",
            "dict_values(['26', 'Private', '162302', 'Some-college', '10', 'Never-married', 'Machine-op-inspct', 'Not-in-family', 'Asian-Pac-Islander', 'Male', '0', '0', '40', 'Philippines', '<=50K'])\n",
            "26\n"
          ]
        }
      ],
      "source": [
        "print(len(data))\n",
        "print(data[0])\n",
        "print(len(data[0]))\n",
        "print(data[0].keys())\n",
        "print(data[0].values())\n",
        "print(data[0]['age'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NyNQ5j-stRO0"
      },
      "outputs": [],
      "source": [
        "def calculate_age_bins(data):\n",
        "    # TODO: return bins as a list of five values where each value represent the number of people in each range of `[0, 20), [20, 40), [40, 60), [60, 80), [80, 100)`\n",
        "    # actual intervals from piazza (0,20] , (20,40], (40, 60], (60, 80], (80,100]\n",
        "    bins = [0,0,0,0,0]\n",
        "    for d in data:\n",
        "        age = int(d['age'])\n",
        "        if age <= 20:\n",
        "            bins[0]+=1\n",
        "        elif age <= 40:\n",
        "            bins[1]+=1\n",
        "        elif age <= 60:\n",
        "            bins[2]+=1\n",
        "        elif age <= 80:\n",
        "            bins[3]+=1\n",
        "        else:\n",
        "            bins[4]+=1    \n",
        "    return bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "skf-ZYAztmpS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass: Data Loading and Analysis [5 points]\n"
          ]
        }
      ],
      "source": [
        "def test_calculate_age_bins():\n",
        "    bins = calculate_age_bins(data)\n",
        "    assert sum(bins) == len(data)\n",
        "    assert bins == [427, 4785, 4147, 624, 17]\n",
        "test_calculate_age_bins()\n",
        "print('Pass: Data Loading and Analysis [5 points]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1QfcRnZx4wF"
      },
      "source": [
        "### **6. Evaluate Accuracy [10 points]**###\n",
        "\n",
        "Before training your model, let's think about how can you evaluate the accuracy of your prediction. Generally speaking, accuracy quantifies how well your model is doing. As a standard convention, 0.5 is used as a threshold to classify predicted values, i.e., if the predicted real-valued output is greater or equal to 0.5, the output is considered `True`, and is considered `False` otherwise.\n",
        "\n",
        " Modify the `accuracy` function to calculate your accuracy on a dataset given a list of data points and the associated predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IL2Zdrj_x77-"
      },
      "outputs": [],
      "source": [
        "def accuracy(data, predictions):\n",
        "    # TODO: Calculate accuracy of predictions on data\n",
        "    correct = 0\n",
        "    for i in range(len(data)):\n",
        "        guess = False\n",
        "        if predictions[i] >= .5:\n",
        "            guess = True\n",
        "        if data[i]['label'] == guess:\n",
        "            correct +=1\n",
        "    return float(correct)/len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CW8dhXum1lob"
      },
      "outputs": [],
      "source": [
        "def extract_features(raw):\n",
        "    data = []\n",
        "    for r in raw:\n",
        "        point = {}\n",
        "        point[\"label\"] = (r['income'] == '>50K')\n",
        "\n",
        "        features = []\n",
        "        features.append(1.)\n",
        "        features.append(float(r['age'])/100)\n",
        "        features.append(float(r['education_num'])/20)\n",
        "        features.append(r['marital'] == 'Married-civ-spouse')\n",
        "        point['features'] = features\n",
        "        data.append(point)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KE9IhXRC1X5A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pass: Accuracy [10 points]\n"
          ]
        }
      ],
      "source": [
        "def test_accuracy(fn):\n",
        "    load_data = load_adult_train_data(fn)\n",
        "    data = extract_features(load_data)\n",
        "    a = accuracy(data, [0]*len(data))\n",
        "    assert abs(a - 0.5) < 1e-7\n",
        "test_accuracy(fp_data)\n",
        "print('Pass: Accuracy [10 points]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY4p5ROoyIwV"
      },
      "source": [
        "### **7. Train Your Model via Batched Gradient-based Optimization [30 points]**###\n",
        "For training your logistic regression model, you need to implement batched gradient ascent. That is, you need to iteratively update the model's weights by computing the gradient while incorporating regularization.\n",
        "\n",
        "Use the update rule from class to adjust the model's weights. The update rule from class is given below:\n",
        "\n",
        "$$w_i^{(t+1)} \\leftarrow w_i^{(t)} - \\eta \\lambda w_i^{(t)} + \\eta \\sum_l X_i^l (Y^l -\\hat P(Y^l=1| X^l, W))$$\n",
        "\n",
        "The training should run for some number of `epochs`. An epoch refers to a full pass over the dataset. In practice it is easier (and more statistically valid) to sample randomly with replacement. Thus, an epoch just means examining `N` data points where `N` is the number of points in your training data.\n",
        "\n",
        "Fill in the `train` function to train your model. You should use logistic regression with regularization where `rate` is the learning rate and `lam` is the regularization parameter.\n",
        "\n",
        "To get a more accurate evaluation, you can modify `load_adult_train_data()` and `load_adult_valid_data()` to use different training and validation sets by splitting your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8_MmxkPtyRqk"
      },
      "outputs": [],
      "source": [
        "def initialize_model(k):\n",
        "    return [random.gauss(0, 1) for x in range(k)]\n",
        "\n",
        "# TODO: Train model using training data\n",
        "def train(data, epochs, rate, lam):\n",
        "    model = initialize_model(len(data[0]['features']))\n",
        "    starting_rate = rate\n",
        "    for e in range(epochs):\n",
        "        rate = starting_rate * exp(-.05 * e)\n",
        "        new_model = [0] * len(data[0]['features'])\n",
        "        for i in range(len(model)):\n",
        "            new_model[i] = model[i] - (rate * lam * model[i])\n",
        "            gradient = 0\n",
        "            for d in data:\n",
        "                gradient += (d['features'][i] * (d['label'] - predict(model, d)))\n",
        "            new_model[i] += (rate * gradient)\n",
        "        model = new_model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUKzKdKrydLn"
      },
      "source": [
        "### **8. Feature Engineering [20 points]**###\n",
        "\n",
        "Feature engineering (or feature extraction) is the process of extracting \"better\" features (characteristics, properties, attributes) from raw data. Good feature engineering is often the key to making good machine learning models. The motivation is to use these extra features to improve the quality of results. Add more feature extraction rules to help improve your model's performance. By definition, this is very open ended and so be creative and experiment to find features that work well with your model.\n",
        "\n",
        "Take a look at the feature extracting code in `extract_features`, and at the raw data in `adult.data`. Right now, your model is only considering age, education, and one possible marital status. In that sense, it is somewhat restrictive and thus \"good\" feature engineering can help improve the performance of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LIqwzBKRyh7X"
      },
      "outputs": [],
      "source": [
        "def extract_features(raw):\n",
        "    data = []\n",
        "    for r in raw:\n",
        "        point = {}\n",
        "        point[\"label\"] = (r['income'] == '>50K')\n",
        "\n",
        "        features = []\n",
        "        features.append(1.)\n",
        "        features.append(float(r['age'])/100)\n",
        "        features.append(float(r['education_num'])/20)\n",
        "        features.append(r['marital'] == 'Married-civ-spouse')\n",
        "        #TODO: Add more feature extraction rules here!\n",
        "        features.append((r['sex'] == 'Male'))\n",
        "        features.append(float(r['hr_per_week']) >= 40)\n",
        "        features.append(r['education'] == '9th' or r['education'] == '10th' or r['education'] == '11th' or r['education'] == '12th' or r['education'] == 'HS-GRAD')\n",
        "        features.append(float(r['capital_gain']) > 0)\n",
        "        point['features'] = features\n",
        "        data.append(point)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylr1ze1gy1HZ"
      },
      "source": [
        "### **9. Fine-tune Your Submission [15 points]**###\n",
        "\n",
        "Fine-tune your `submission` function to train your final model. You should change your feature extraction and training code to produce the best model you can. Try different learning rates and regularization parameters and see how do they compare. Often it is good to start with a high learning rate and decrease it over time. This is known as learning rate annealing. The way learning rate evolves over time during optimization can be defined by a schedule. Feel free to try various learning rate annealing schedules and observe their effects to figure out what works best given your creative feature engineering. If so, you may need to modify the `train` function to implement this. Your `submission` function should finish execution in no more than 2 minutes, you will get zero points otherwise.\n",
        "\n",
        "Your final model will be trained on the full training data and tested on an independent validation dataset that you don't have access to. Your grade for this section will be based on your performance relative to a baseline model we obtained during our in-house testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "--bZEA4GrB8_"
      },
      "outputs": [],
      "source": [
        "# TODO: Tune your parameters for final submission\n",
        "def submission(data):\n",
        "    random.seed(1)\n",
        "    return train(data, 50, 3e-3, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8CM8gcd13TQS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.7838\n",
            "Validation Accuracy: 0.7838\n"
          ]
        }
      ],
      "source": [
        "def test_submission(fn):\n",
        "    train_data = extract_features(load_adult_train_data(fn))\n",
        "    valid_data = extract_features(load_adult_valid_data(fn))\n",
        "    model = submission(train_data)\n",
        "    predictions = [predict(model, p) for p in train_data]\n",
        "    print(\"Training Accuracy:\", accuracy(train_data, predictions))\n",
        "    predictions = [predict(model, p) for p in valid_data]\n",
        "    print(\"Validation Accuracy:\", accuracy(valid_data, predictions))\n",
        "test_submission(fp_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTvn6PIaPy6r"
      },
      "source": [
        "### **10. Acknowledgments**###\n",
        "\n",
        "The data file is adapted from the adult dataset originally from the UCI repository and later released <a href=\"https://www.cs.toronto.edu/~delve/data/adult/adultDetail.html\">here</a>. We performed class balancing and downsampling for robust performance evaluation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
